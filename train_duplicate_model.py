#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π
"""

import pandas as pd
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import xgboost as xgb
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pickle
import joblib
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class DuplicateNewsModel:
    """–ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π"""
    
    def __init__(self, model_name='paraphrase-multilingual-mpnet-base-v2'):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        
        Args:
            model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ sentence-transformers
        """
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {model_name}...")
        self.embedding_model = SentenceTransformer(model_name)
        self.classifier = None
        self.feature_names = []
        
    def extract_features(self, text1, text2):
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            text1: –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            
        Returns:
            numpy array —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
        """
        # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        emb1 = self.embedding_model.encode([text1])[0]
        emb2 = self.embedding_model.encode([text2])[0]
        
        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        cosine_sim = cosine_similarity([emb1], [emb2])[0][0]
        
        # –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        euclidean_dist = np.linalg.norm(emb1 - emb2)
        
        # –ú–∞–Ω—Ö—ç—Ç—Ç–µ–Ω—Å–∫–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
        manhattan_dist = np.sum(np.abs(emb1 - emb2))
        
        # –î–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤
        len1 = len(text1)
        len2 = len(text2)
        len_ratio = min(len1, len2) / max(len1, len2) if max(len1, len2) > 0 else 0
        len_diff = abs(len1 - len2)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
        words1 = len(text1.split())
        words2 = len(text2.split())
        word_ratio = min(words1, words2) / max(words1, words2) if max(words1, words2) > 0 else 0
        word_diff = abs(words1 - words2)
        
        # –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
        set1 = set(text1.lower().split())
        set2 = set(text2.lower().split())
        jaccard = len(set1.intersection(set2)) / len(set1.union(set2)) if len(set1.union(set2)) > 0 else 0
        
        # –ü—Ä–æ—Å—Ç—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        common_chars = sum((text1.lower().count(c) + text2.lower().count(c)) / 2 
                          for c in set(text1.lower() + text2.lower()))
        
        features = np.array([
            cosine_sim,
            euclidean_dist,
            manhattan_dist,
            len_ratio,
            len_diff,
            word_ratio,
            word_diff,
            jaccard,
            common_chars
        ])
        
        return features
    
    def prepare_features(self, df, text_col1='text1', text_col2='text2', batch_size=32):
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤—Å–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            df: pandas DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
            text_col1: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –ø–µ—Ä–≤—ã–º —Ç–µ–∫—Å—Ç–æ–º
            text_col2: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—Ç–æ—Ä—ã–º —Ç–µ–∫—Å—Ç–æ–º
            batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            
        Returns:
            numpy array —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–º–µ—Ä–æ–≤
        """
        print("–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞–Ω–Ω—ã—Ö...")
        features_list = []
        
        for idx in tqdm(range(len(df)), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞"):
            text1 = str(df.iloc[idx][text_col1])
            text2 = str(df.iloc[idx][text_col2])
            features = self.extract_features(text1, text2)
            features_list.append(features)
        
        self.feature_names = [
            'cosine_similarity',
            'euclidean_distance',
            'manhattan_distance',
            'length_ratio',
            'length_diff',
            'word_ratio',
            'word_diff',
            'jaccard_similarity',
            'common_chars'
        ]
        
        return np.array(features_list)
    
    def train(self, X_train, y_train, classifier_type='xgboost'):
        """
        –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        
        Args:
            X_train: –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            y_train: –º–µ—Ç–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            classifier_type: —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ('logistic', 'random_forest', 'gradient_boosting', 'xgboost')
        """
        print(f"–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {classifier_type}...")
        
        if classifier_type == 'logistic':
            self.classifier = LogisticRegression(max_iter=1000, random_state=42)
        elif classifier_type == 'random_forest':
            self.classifier = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
        elif classifier_type == 'gradient_boosting':
            self.classifier = GradientBoostingClassifier(n_estimators=100, random_state=42)
        elif classifier_type == 'xgboost':
            self.classifier = xgb.XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42,
                n_jobs=-1
            )
        else:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞: {classifier_type}")
        
        self.classifier.fit(X_train, y_train)
        print("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    
    def predict(self, text1, text2):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª—è –ø–∞—Ä—ã —Ç–µ–∫—Å—Ç–æ–≤
        
        Args:
            text1: –ø–µ—Ä–≤—ã–π —Ç–µ–∫—Å—Ç
            text2: –≤—Ç–æ—Ä–æ–π —Ç–µ–∫—Å—Ç
            
        Returns:
            tuple (prediction, probability)
        """
        if self.classifier is None:
            raise ValueError("–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞! –í—ã–∑–æ–≤–∏—Ç–µ train() —Å–Ω–∞—á–∞–ª–∞.")
        
        features = self.extract_features(text1, text2)
        features = features.reshape(1, -1)
        
        prediction = self.classifier.predict(features)[0]
        probability = self.classifier.predict_proba(features)[0]
        
        return prediction, probability
    
    def evaluate(self, X_test, y_test):
        """
        –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            X_test: –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            y_test: –º–µ—Ç–∫–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            
        Returns:
            dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        print("\n–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        y_pred = self.classifier.predict(X_test)
        y_pred_proba = self.classifier.predict_proba(X_test)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred),
            'recall': recall_score(y_test, y_pred),
            'f1': f1_score(y_test, y_pred)
        }
        
        print("\n" + "="*60)
        print("–ú–ï–¢–†–ò–ö–ò –ú–û–î–ï–õ–ò")
        print("="*60)
        print(f"Accuracy:  {metrics['accuracy']:.4f}")
        print(f"Precision: {metrics['precision']:.4f}")
        print(f"Recall:    {metrics['recall']:.4f}")
        print(f"F1-Score:  {metrics['f1']:.4f}")
        
        print("\n" + "="*60)
        print("–ú–ê–¢–†–ò–¶–ê –û–®–ò–ë–û–ö")
        print("="*60)
        cm = confusion_matrix(y_test, y_pred)
        print(cm)
        
        print("\n" + "="*60)
        print("–ü–û–î–†–û–ë–ù–´–ô –û–¢–ß–ï–¢")
        print("="*60)
        print(classification_report(y_test, y_pred, target_names=['–ù–µ –¥—É–±–ª–∏–∫–∞—Ç—ã', '–î—É–±–ª–∏–∫–∞—Ç—ã']))
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
        if hasattr(self.classifier, 'feature_importances_'):
            print("\n" + "="*60)
            print("–í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í")
            print("="*60)
            importances = self.classifier.feature_importances_
            indices = np.argsort(importances)[::-1]
            
            for i, idx in enumerate(indices):
                print(f"{i+1}. {self.feature_names[idx]}: {importances[idx]:.4f}")
        
        return metrics, y_pred, y_pred_proba
    
    def save(self, model_path):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        """
        model_path = Path(model_path)
        model_path.parent.mkdir(parents=True, exist_ok=True)
        
        model_data = {
            'classifier': self.classifier,
            'feature_names': self.feature_names,
            'embedding_model_name': self.embedding_model.__class__.__name__
        }
        
        joblib.dump(model_data, model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path}")
    
    def load(self, model_path):
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        model_data = joblib.load(model_path)
        self.classifier = model_data['classifier']
        self.feature_names = model_data['feature_names']
        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_path}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
    DATA_PATH = Path(__file__).parent / "unified_news_pairs.csv"
    MODEL_DIR = Path(__file__).parent / "models"
    MODEL_DIR.mkdir(exist_ok=True)
    
    print("="*60)
    print("–û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –û–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –î–£–ë–õ–ò–ö–ê–¢–û–í –ù–û–í–û–°–¢–ï–ô")
    print("="*60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {DATA_PATH}...")
    df = pd.read_csv(DATA_PATH)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
    print(f"   –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫:")
    print(df['label'].value_counts())
    print(f"   –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {df['label'].sum()} ({df['label'].sum()/len(df)*100:.2f}%)")
    print(f"   –ù–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {(1-df['label']).sum()} ({(1-df['label']).sum()/len(df)*100:.2f}%)")
    
    # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–∑—å–º–µ–º –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ (–º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
    # if len(df) > 10000:
    #    print(f"\n‚ö†Ô∏è  –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∏–∑ 10000 –∑–∞–ø–∏—Å–µ–π")
    #    df = df.sample(n=10000, random_state=RANDOM_STATE)
    
    # 2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    print("\n2. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
    model = DuplicateNewsModel()
    
    # 3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    print("\n3. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
    X = model.prepare_features(df)
    y = df['label'].values
    
    # 4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö
    print("\n4. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø–æ–ª–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ...")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {len(X)}")
    print(f"   –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {y.sum()} ({y.sum()/len(y)*100:.1f}%)")
    print(f"   –ù–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {(1-y).sum()} ({(1-y).sum()/len(y)*100:.1f}%)")
    
    classifiers = ['logistic', 'random_forest', 'gradient_boosting', 'xgboost']
    
    for clf_type in classifiers:
        print("\n" + "="*60)
        print(f"–û–ë–£–ß–ï–ù–ò–ï: {clf_type.upper()}")
        print("="*60)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model_instance = DuplicateNewsModel()
        model_instance.feature_names = model.feature_names
        model_instance.train(X, y, classifier_type=clf_type)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        model_path = MODEL_DIR / f"duplicate_model_{clf_type}.joblib"
        model_instance.save(model_path)
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_path.name}")
    
    # 5. –ò—Ç–æ–≥
    print("\n" + "="*60)
    print("‚úÖ –í–°–ï –ú–û–î–ï–õ–ò –û–ë–£–ß–ï–ù–´ –ò –°–û–•–†–ê–ù–ï–ù–´")
    print("="*60)
    print(f"\nüìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(classifiers)} –º–æ–¥–µ–ª–µ–π –≤ {MODEL_DIR}")
    print("\nüí° –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
    print("   python validate_models.py")
    
    # 6. –ë—ã—Å—Ç—Ä–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏)
    print("\n" + "="*60)
    print("–ë–´–°–¢–†–ê–Ø –ü–†–û–í–ï–†–ö–ê –†–ê–ë–û–¢–û–°–ü–û–°–û–ë–ù–û–°–¢–ò")
    print("="*60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º XGBoost –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    test_model = DuplicateNewsModel()
    test_model.load(MODEL_DIR / "duplicate_model_xgboost.joblib")
    
    # –ü—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    test_examples = [
        ("–ü—Ä–µ–∑–∏–¥–µ–Ω—Ç –ø–æ–¥–ø–∏—Å–∞–ª –Ω–æ–≤—ã–π –∑–∞–∫–æ–Ω –æ –Ω–∞–ª–æ–≥–∞—Ö.", 
         "–ì–ª–∞–≤–∞ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–∞ —É—Ç–≤–µ—Ä–¥–∏–ª –Ω–∞–ª–æ–≥–æ–≤—ã–π –∑–∞–∫–æ–Ω.", 
         True),
        ("–ó–∞–≤—Ç—Ä–∞ –±—É–¥–µ—Ç –¥–æ–∂–¥—å.", 
         "–í –†–æ—Å—Å–∏–∏ –æ—Ç–∫—Ä—ã–ª—Å—è –Ω–æ–≤—ã–π –∑–∞–≤–æ–¥ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤—É –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π.", 
         False),
        ("–ö–æ–º–ø–∞–Ω–∏—è Apple –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ –Ω–æ–≤—ã–π iPhone.", 
         "Apple –∞–Ω–æ–Ω—Å–∏—Ä–æ–≤–∞–ª–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –º–æ–¥–µ–ª—å iPhone.", 
         True),
    ]
    
    print("\n–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ XGBoost –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö:")
    for i, (text1, text2, expected) in enumerate(test_examples, 1):
        prediction, probability = test_model.predict(text1, text2)
        status = '‚úÖ' if (prediction == 1) == expected else '‚ùå'
        result = '–î—É–±–ª–∏–∫–∞—Ç' if prediction == 1 else '–ù–µ –¥—É–±–ª–∏–∫–∞—Ç'
        print(f"  {i}. {status} –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {result} (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å: {probability[1]:.2%})")
    
    print("\n" + "="*60)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*60)
    print("\nüìä –î–ª—è –ø–æ–ª–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
    print("   python validate_models.py")


if __name__ == "__main__":
    main()

