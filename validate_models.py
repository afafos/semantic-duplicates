#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ç–∫–∞—Ö
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ agreed_annotations_osokina_popov.json –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
"""

import json
from pathlib import Path
from train_duplicate_model import DuplicateNewsModel
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
import numpy as np
from tqdm import tqdm


def parse_text_pair(text):
    """
    –ü–∞—Ä—Å–∏—Ç —Ç–µ–∫—Å—Ç —Å –¥–≤—É–º—è –Ω–æ–≤–æ—Å—Ç—è–º–∏, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω—ã–º–∏ –º–∞—Ä–∫–µ—Ä–∞–º–∏
    
    Args:
        text: —Å—Ç—Ä–æ–∫–∞ —Å –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏
        
    Returns:
        tuple (text1, text2)
    """
    parts = text.split("----------–¢–ï–ö–°–¢ 2----------")
    
    if len(parts) != 2:
        return None, None
    
    text1 = parts[0].replace("----------–¢–ï–ö–°–¢ 1----------", "").strip()
    text2 = parts[1].strip()
    
    return text1, text2


def load_annotations(json_path):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–∏
    
    Args:
        json_path: –ø—É—Ç—å –∫ JSON —Ñ–∞–π–ª—É
        
    Returns:
        list –∏–∑ —Å–ª–æ–≤–∞—Ä–µ–π —Å –ø–∞—Ä–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤ –∏ –º–µ—Ç–∫–∞–º–∏
    """
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    parsed_data = []
    
    for item in data:
        text1, text2 = parse_text_pair(item['text'])
        
        if text1 is None or text2 is None:
            continue
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –º–µ—Ç–∫—É –≤ –±–∏–Ω–∞—Ä–Ω—É—é (0 –∏–ª–∏ 1)
        label = 1 if item['agreed_label'] == 'are_duplicates' else 0
        
        parsed_data.append({
            'id': item['id'],
            'text1': text1,
            'text2': text2,
            'label': label,
            'annotators': item['annotators']
        })
    
    return parsed_data


def evaluate_model_on_annotations(model, data, sample_size=None):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å DuplicateNewsModel
        data: —Å–ø–∏—Å–æ–∫ —Å —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–∏ —Ç–µ–∫—Å—Ç–æ–≤
        sample_size: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (None = –≤—Å–µ)
        
    Returns:
        dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è–º–∏
    """
    if sample_size:
        data = data[:sample_size]
    
    y_true = []
    y_pred = []
    y_pred_proba = []
    
    print(f"–û—Ü–µ–Ω–∫–∞ –Ω–∞ {len(data)} –ø—Ä–∏–º–µ—Ä–∞—Ö...")
    
    for item in tqdm(data, desc="–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ"):
        try:
            prediction, probability = model.predict(item['text1'], item['text2'])
            
            y_true.append(item['label'])
            y_pred.append(prediction)
            y_pred_proba.append(probability[1])  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–∞
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–ª—è ID {item['id']}: {e}")
            continue
    
    # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
    metrics = {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred, zero_division=0),
        'recall': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        'confusion_matrix': confusion_matrix(y_true, y_pred),
        'y_true': y_true,
        'y_pred': y_pred,
        'y_pred_proba': y_pred_proba
    }
    
    return metrics


def print_model_metrics(model_name, metrics):
    """
    –ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏—Ç –º–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏
    
    Args:
        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        metrics: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
    """
    print(f"\n{'='*80}")
    print(f"–ú–û–î–ï–õ–¨: {model_name}")
    print(f"{'='*80}")
    
    print(f"\nüìä –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"   Accuracy:  {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"   Precision: {metrics['precision']:.4f} ({metrics['precision']*100:.2f}%)")
    print(f"   Recall:    {metrics['recall']:.4f} ({metrics['recall']*100:.2f}%)")
    print(f"   F1-Score:  {metrics['f1']:.4f} ({metrics['f1']*100:.2f}%)")
    
    print(f"\nüìà –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
    cm = metrics['confusion_matrix']
    print(f"                   Predicted")
    print(f"                   Not Dup    Duplicate")
    print(f"   Actual Not Dup    {cm[0][0]:<8}   {cm[0][1]:<8}")
    print(f"   Actual Duplicate  {cm[1][0]:<8}   {cm[1][1]:<8}")
    
    # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
    false_positives = cm[0][1]
    false_negatives = cm[1][0]
    total_errors = false_positives + false_negatives
    total_samples = len(metrics['y_true'])
    
    print(f"\n‚ùå –û—à–∏–±–∫–∏:")
    print(f"   False Positives (–ª–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã):  {false_positives} ({false_positives/total_samples*100:.2f}%)")
    print(f"   False Negatives (–ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥—É–±–ª.): {false_negatives} ({false_negatives/total_samples*100:.2f}%)")
    print(f"   –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {total_errors} –∏–∑ {total_samples} ({total_errors/total_samples*100:.2f}%)")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    print("="*80)
    print("–í–ê–õ–ò–î–ê–¶–ò–Ø –ú–û–î–ï–õ–ï–ô –ù–ê –°–û–ì–õ–ê–°–û–í–ê–ù–ù–´–• –†–ê–ó–ú–ï–¢–ö–ê–•")
    print("="*80)
    
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    annotations_path = Path(__file__).parent / "agreed_annotations_osokina_popov.json"
    models_dir = Path(__file__).parent / "models"
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ñ–∞–π–ª–æ–≤
    if not annotations_path.exists():
        print(f"\n‚ùå –û—à–∏–±–∫–∞: –§–∞–π–ª —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {annotations_path}")
        return
    
    if not models_dir.exists():
        print(f"\n‚ùå –û—à–∏–±–∫–∞: –ü–∞–ø–∫–∞ —Å –º–æ–¥–µ–ª—è–º–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {models_dir}")
        print("   –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏: python train_duplicate_model.py")
        return
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print(f"\n1. –ó–∞–≥—Ä—É–∑–∫–∞ –∞–Ω–Ω–æ—Ç–∞—Ü–∏–π –∏–∑ {annotations_path.name}...")
    data = load_annotations(annotations_path)
    print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(data)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –º–µ—Ç–∫–∞–º
    duplicates_count = sum(1 for item in data if item['label'] == 1)
    not_duplicates_count = len(data) - duplicates_count
    print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
    print(f"   –î—É–±–ª–∏–∫–∞—Ç—ã:     {duplicates_count} ({duplicates_count/len(data)*100:.1f}%)")
    print(f"   –ù–µ –¥—É–±–ª–∏–∫–∞—Ç—ã:  {not_duplicates_count} ({not_duplicates_count/len(data)*100:.1f}%)")
    
    # –û–ø—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    SAMPLE_SIZE = None  # –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ —á–∏—Å–ª–æ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 100) –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
    
    if SAMPLE_SIZE and len(data) > SAMPLE_SIZE:
        print(f"\n‚ö†Ô∏è  –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ: {SAMPLE_SIZE} –ø—Ä–∏–º–µ—Ä–æ–≤")
        data = data[:SAMPLE_SIZE]
    
    # 2. –ü–æ–∏—Å–∫ –º–æ–¥–µ–ª–µ–π
    print(f"\n2. –ü–æ–∏—Å–∫ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –≤ {models_dir}...")
    model_files = list(models_dir.glob("duplicate_model_*.joblib"))
    
    if not model_files:
        print(f"‚ùå –û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(model_files)}")
    for mf in sorted(model_files):
        print(f"   - {mf.name}")
    
    # 3. –û—Ü–µ–Ω–∫–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    print(f"\n{'='*80}")
    print("3. –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ï–ô")
    print(f"{'='*80}")
    
    all_results = {}
    
    for model_file in sorted(model_files):
        model_name = model_file.stem.replace("duplicate_model_", "").upper()
        
        try:
            # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            print(f"\n{'='*80}")
            print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {model_name}")
            print(f"{'='*80}")
            
            model = DuplicateNewsModel()
            model.load(model_file)
            
            # –û—Ü–µ–Ω–∫–∞
            metrics = evaluate_model_on_annotations(model, data, SAMPLE_SIZE)
            all_results[model_name] = metrics
            
            # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print_model_metrics(model_name, metrics)
            
        except Exception as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            import traceback
            traceback.print_exc()
    
    # 4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    if len(all_results) > 1:
        print(f"\n{'='*80}")
        print("4. –°–†–ê–í–ù–ï–ù–ò–ï –í–°–ï–• –ú–û–î–ï–õ–ï–ô")
        print(f"{'='*80}")
        
        print(f"\n{'–ú–æ–¥–µ–ª—å':<25} {'Accuracy':<12} {'Precision':<12} {'Recall':<12} {'F1-Score':<12}")
        print("-"*80)
        
        for model_name, metrics in sorted(all_results.items()):
            print(f"{model_name:<25} "
                  f"{metrics['accuracy']:.4f}       "
                  f"{metrics['precision']:.4f}       "
                  f"{metrics['recall']:.4f}       "
                  f"{metrics['f1']:.4f}")
        
        # –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å
        best_model = max(all_results.items(), key=lambda x: x[1]['f1'])
        print(f"\nüèÜ –õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨ (–ø–æ F1-Score): {best_model[0]}")
        print(f"   F1-Score: {best_model[1]['f1']:.4f} ({best_model[1]['f1']*100:.2f}%)")
        print(f"   Accuracy: {best_model[1]['accuracy']:.4f} ({best_model[1]['accuracy']*100:.2f}%)")
    
    # 5. –ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç—á–µ—Ç –¥–ª—è –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
    if all_results:
        best_model_name, best_metrics = max(all_results.items(), key=lambda x: x[1]['f1'])
        
        print(f"\n{'='*80}")
        print(f"5. –ü–û–î–†–û–ë–ù–´–ô –û–¢–ß–ï–¢ –î–õ–Ø –õ–£–ß–®–ï–ô –ú–û–î–ï–õ–ò: {best_model_name}")
        print(f"{'='*80}")
        
        print("\nClassification Report:")
        print(classification_report(
            best_metrics['y_true'], 
            best_metrics['y_pred'],
            target_names=['–ù–µ –¥—É–±–ª–∏–∫–∞—Ç—ã', '–î—É–±–ª–∏–∫–∞—Ç—ã'],
            digits=4
        ))
        
        # –ü—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫
        print(f"\n{'='*80}")
        print("–ü–†–ò–ú–ï–†–´ –û–®–ò–ë–û–ö")
        print(f"{'='*80}")
        
        errors = []
        for i, (true_label, pred_label, prob) in enumerate(zip(
            best_metrics['y_true'], 
            best_metrics['y_pred'],
            best_metrics['y_pred_proba']
        )):
            if true_label != pred_label:
                errors.append({
                    'index': i,
                    'true_label': true_label,
                    'pred_label': pred_label,
                    'probability': prob,
                    'item': data[i]
                })
        
        if errors:
            print(f"\n–í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {len(errors)}")
            print(f"\n–ü–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫:\n")
            
            for i, error in enumerate(errors[:5], 1):
                item = error['item']
                print(f"{i}. ID: {item['id']}")
                print(f"   –ò—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {'–î—É–±–ª–∏–∫–∞—Ç' if error['true_label'] == 1 else '–ù–µ –¥—É–±–ª–∏–∫–∞—Ç'}")
                print(f"   –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:   {'–î—É–±–ª–∏–∫–∞—Ç' if error['pred_label'] == 1 else '–ù–µ –¥—É–±–ª–∏–∫–∞—Ç'}")
                print(f"   –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥—É–±–ª–∏–∫–∞—Ç–∞: {error['probability']:.2%}")
                print(f"   –¢–µ–∫—Å—Ç 1: {item['text1'][:100]}...")
                print(f"   –¢–µ–∫—Å—Ç 2: {item['text2'][:100]}...")
                print()
        else:
            print("\n‚úÖ –û—à–∏–±–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ! –ú–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–∞!")
    
    print(f"\n{'='*80}")
    print("‚úÖ –í–ê–õ–ò–î–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print(f"{'='*80}")


if __name__ == "__main__":
    main()

