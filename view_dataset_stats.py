#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ—Å–º–æ—Ç—Ä–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É –∏—Å—Ö–æ–¥–Ω–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É.
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫.

–î–∞—Ç–∞—Å–µ—Ç—ã:
  - merionum/ru_paraphraser (HuggingFace)
  - GEM/opusparcus (RU) (HuggingFace)
  - cointegrated/ru-paraphrase-NMT-Leipzig (HuggingFace)
  - viacheslavshalamov/russian-news-paraphrases-2020 (Kaggle)

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python view_dataset_stats.py
"""

from __future__ import annotations
import os
import shutil
import sys
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Optional

import pandas as pd
from datasets import load_dataset
from huggingface_hub import hf_hub_download
from kaggle.api.kaggle_api_extended import KaggleApi


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –£—Ç–∏–ª–∏—Ç—ã –¥–ª—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
try:
    from dotenv import load_dotenv as _load_dotenv
except ImportError:
    _load_dotenv = None


def setup_auth():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –¥–ª—è HuggingFace –∏ Kaggle."""
    if _load_dotenv is not None:
        env_path = Path(__file__).with_name(".env")
        if env_path.exists():
            _load_dotenv(env_path)
    
    # HF authentication
    hf_token = os.getenv("HF_TOKEN")
    if hf_token:
        try:
            from huggingface_hub import login
            login(token=hf_token, add_to_git_credential=False)
        except Exception:
            pass
    
    # Kaggle authentication
    kaggle_json_path = Path(__file__).with_name("kaggle.json")
    if kaggle_json_path.exists():
        kaggle_dir = Path.home() / ".kaggle"
        kaggle_dir.mkdir(exist_ok=True)
        target = kaggle_dir / "kaggle.json"
        if not target.exists():
            shutil.copy(kaggle_json_path, target)
            target.chmod(0o600)


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def print_separator(char="=", length=80):
    """–ü–µ—á–∞—Ç–∞–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å."""
    print(char * length)


def print_dataset_stats(name: str, df: pd.DataFrame, label_info: Optional[dict] = None):
    """
    –ü–µ—á–∞—Ç–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–∞—Ç–∞—Å–µ—Ç—É.
    
    Args:
        name: –ù–∞–∑–≤–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
        df: DataFrame —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ (–¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏)
        label_info: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Ç–∫–∞—Ö
    """
    print_separator()
    print(f"üìä {name}")
    print_separator()
    
    total_records = len(df)
    print(f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–∞–∑–º–µ—Ç–∫–µ –º–µ—Ç–æ–∫
    if label_info:
        print(f"\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–µ—Ç–∫–∞—Ö:")
        for key, value in label_info.items():
            print(f"  {key}: {value}")
    
    print()


def analyze_ru_paraphraser():
    """–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ merionum/ru_paraphraser."""
    try:
        print("\nüîç –ó–∞–≥—Ä—É–∑–∫–∞ merionum/ru_paraphraser...")
        ds = load_dataset("merionum/ru_paraphraser")
        
        all_data = []
        for split in ("train", "test"):
            d = ds[split].to_pandas()
            d['split'] = split
            all_data.append(d)
        
        df = pd.concat(all_data, ignore_index=True)
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–∫
        label_col = None
        for col in ['label', 'class', 'gold_label']:
            if col in df.columns:
                label_col = col
                break
        
        if label_col is None:
            label_col = df.columns[2] if len(df.columns) > 2 else df.columns[-1]
        
        # –ü–æ–¥—Å—á—ë—Ç –º–µ—Ç–æ–∫
        labels = df[label_col]
        label_counts = labels.value_counts().sort_index()
        
        label_info = {
            "–ö–æ–ª–æ–Ω–∫–∞ —Å –º–µ—Ç–∫–∞–º–∏": label_col,
            "–†–∞–∑–º–µ—Ç–∫–∞": "1 = —Ç–æ—á–Ω—ã–π –ø–∞—Ä–∞—Ñ—Ä–∞–∑ (–¥—É–±–ª–∏–∫–∞—Ç), 0 = near (–ø–æ—Ö–æ–∂–∏–π), -1 = –Ω–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑",
            "–í—Å–µ–≥–æ —Ä–∞–∑–º–µ—á–µ–Ω–æ": f"{len(labels):,} (100%)",
            ""  : "",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫": ""
        }
        
        for label_val, count in label_counts.items():
            percentage = (count / len(labels)) * 100
            label_name = {
                1: "—Ç–æ—á–Ω—ã–π –ø–∞—Ä–∞—Ñ—Ä–∞–∑ (–¥—É–±–ª–∏–∫–∞—Ç)",
                0: "near (–ø–æ—Ö–æ–∂–∏–π)",
                -1: "–Ω–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑"
            }.get(label_val, f"–º–µ—Ç–∫–∞ {label_val}")
            label_info[f"  {label_val} ({label_name})"] = f"{count:,} ({percentage:.2f}%)"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–ª–∏—Ç–∞–º
        split_stats = df['split'].value_counts()
        label_info["  "] = ""
        label_info["–ü–æ —Å–ø–ª–∏—Ç–∞–º"] = ""
        for split, count in split_stats.items():
            percentage = (count / len(df)) * 100
            label_info[f"  {split}"] = f"{count:,} ({percentage:.2f}%)"
        
        print_dataset_stats("merionum/ru_paraphraser", df, label_info)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ ru_paraphraser: {e}")
        print()


def analyze_opusparcus():
    """–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ GEM/opusparcus (RU)."""
    try:
        print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ GEM/opusparcus (RU)...")
        val_path = hf_hub_download(
            repo_id="GEM/opusparcus",
            filename="validation.jsonl",
            repo_type="dataset",
            token=False,
        )
        test_path = hf_hub_download(
            repo_id="GEM/opusparcus",
            filename="test.jsonl",
            repo_type="dataset",
            token=False,
        )
        
        ds = load_dataset("json", data_files={"validation": val_path, "test": test_path})
        
        all_data = []
        for split in ("validation", "test"):
            d = ds[split].to_pandas()
            d['split'] = split
            all_data.append(d)
        
        df = pd.concat(all_data, ignore_index=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫
        df_all = df.copy()
        if "lang" in df.columns:
            df = df[df["lang"] == "ru"].copy()
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–æ–∫ (annot_score)
        df["annot_score"] = pd.to_numeric(df["annot_score"], errors="coerce")
        score_counts = df["annot_score"].value_counts().sort_index()
        
        label_info = {
            "–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π (–≤—Å–µ —è–∑—ã–∫–∏)": f"{len(df_all):,}",
            "–ó–∞–ø–∏—Å–µ–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º": f"{len(df):,} (100% –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞)",
            "–ö–æ–ª–æ–Ω–∫–∞ —Å –º–µ—Ç–∫–∞–º–∏": "annot_score",
            "–†–∞–∑–º–µ—Ç–∫–∞": "1.0-2.0 = –Ω–µ –¥—É–±–ª–∏–∫–∞—Ç, 3.0-4.0 = –¥—É–±–ª–∏–∫–∞—Ç, 2.5 –∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è",
            "": "",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ annot_score": ""
        }
        
        for score, count in score_counts.items():
            percentage = (count / len(df)) * 100
            if score <= 2.0:
                category = "(–Ω–µ –¥—É–±–ª–∏–∫–∞—Ç)"
            elif score >= 3.0:
                category = "(–¥—É–±–ª–∏–∫–∞—Ç)"
            else:
                category = "(–∏—Å–∫–ª—é—á–∞–µ—Ç—Å—è)"
            label_info[f"  {score:.1f} {category}"] = f"{count:,} ({percentage:.2f}%)"
        
        # –ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ñ–∏–ª—å—Ç—Ä–æ–≤
        keep = (df["annot_score"] >= 3.0) | (df["annot_score"] <= 2.0)
        df_filtered = df[keep]
        pos = (df["annot_score"] >= 3.0).sum()
        neg = (df["annot_score"] <= 2.0).sum()
        excluded = len(df) - len(df_filtered)
        
        label_info["  "] = ""
        label_info["–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –ø–æ—Ä–æ–≥–æ–≤"] = ""
        label_info["  –î—É–±–ª–∏–∫–∞—Ç—ã (score >= 3.0)"] = f"{pos:,} ({pos/len(df)*100:.2f}%)"
        label_info["  –ù–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (score <= 2.0)"] = f"{neg:,} ({neg/len(df)*100:.2f}%)"
        label_info["  –ò—Å–∫–ª—é—á–µ–Ω–æ (2.0 < score < 3.0)"] = f"{excluded:,} ({excluded/len(df)*100:.2f}%)"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–ª–∏—Ç–∞–º
        split_stats = df['split'].value_counts()
        label_info["   "] = ""
        label_info["–ü–æ —Å–ø–ª–∏—Ç–∞–º (RU)"] = ""
        for split, count in split_stats.items():
            percentage = (count / len(df)) * 100
            label_info[f"  {split}"] = f"{count:,} ({percentage:.2f}%)"
        
        print_dataset_stats("GEM/opusparcus (RU)", df, label_info)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ opusparcus: {e}")
        print()


def analyze_leipzig():
    """–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ cointegrated/ru-paraphrase-NMT-Leipzig."""
    try:
        print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ cointegrated/ru-paraphrase-NMT-Leipzig...")
        ds = load_dataset(
            "cointegrated/ru-paraphrase-NMT-Leipzig",
            data_files={"train": "train.csv", "val": "val.csv", "test": "test.csv"},
        )
        
        all_data = []
        for split in ("train", "val", "test"):
            d = ds[split].to_pandas()
            d['split'] = split
            all_data.append(d)
        
        df = pd.concat(all_data, ignore_index=True)
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
        has_p_good = "p_good" in df.columns
        has_labse = "labse_sim" in df.columns
        
        label_info = {
            "–¢–∏–ø –¥–∞—Ç–∞—Å–µ—Ç–∞": "–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π (—Ç–æ–ª—å–∫–æ –¥—É–±–ª–∏–∫–∞—Ç—ã)",
            "–ú–µ—Ç–æ–¥": "–ü–µ—Ä–µ–≤–æ–¥ RU -> EN -> RU —á–µ—Ä–µ–∑ NMT",
            "–í—Å–µ–≥–æ —Ä–∞–∑–º–µ—á–µ–Ω–æ": f"{len(df):,} (100%, –≤—Å–µ –¥—É–±–ª–∏–∫–∞—Ç—ã)",
            "": "",
            "–ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞": ""
        }
        
        if has_p_good:
            p_good = pd.to_numeric(df["p_good"], errors="coerce")
            label_info["  p_good (–≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–∞)"] = f"—Å—Ä–µ–¥–Ω–µ–µ: {p_good.mean():.3f}, –º–∏–Ω: {p_good.min():.3f}, –º–∞–∫—Å: {p_good.max():.3f}"
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ—Ä–æ–≥–∞–º
            thresholds = [0.7, 0.8, 0.85, 0.9, 0.95]
            for th in thresholds:
                count = (p_good >= th).sum()
                pct = count / len(df) * 100
                label_info[f"    >= {th}"] = f"{count:,} ({pct:.2f}%)"
        
        if has_labse:
            labse_sim = pd.to_numeric(df["labse_sim"], errors="coerce")
            label_info["  labse_sim (LaBSE —Å—Ö–æ–¥—Å—Ç–≤–æ)"] = f"—Å—Ä–µ–¥–Ω–µ–µ: {labse_sim.mean():.3f}, –º–∏–Ω: {labse_sim.min():.3f}, –º–∞–∫—Å: {labse_sim.max():.3f}"
            
            # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –ø–æ—Ä–æ–≥–∞–º
            thresholds = [0.7, 0.8, 0.85, 0.88, 0.9, 0.95]
            for th in thresholds:
                count = (labse_sim >= th).sum()
                pct = count / len(df) * 100
                label_info[f"    >= {th}"] = f"{count:,} ({pct:.2f}%)"
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–ø–ª–∏—Ç–∞–º
        split_stats = df['split'].value_counts()
        label_info["  "] = ""
        label_info["–ü–æ —Å–ø–ª–∏—Ç–∞–º"] = ""
        for split, count in split_stats.items():
            percentage = (count / len(df)) * 100
            label_info[f"  {split}"] = f"{count:,} ({percentage:.2f}%)"
        
        print_dataset_stats("cointegrated/ru-paraphrase-NMT-Leipzig", df, label_info)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ ru-paraphrase-NMT-Leipzig: {e}")
        print()


def analyze_kaggle_news():
    """–ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ viacheslavshalamov/russian-news-paraphrases-2020."""
    try:
        print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ viacheslavshalamov/russian-news-paraphrases-2020 (Kaggle)...")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Kaggle API
        api = KaggleApi()
        api.authenticate()
        
        # –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏
        download_dir = Path(__file__).parent / ".kaggle_cache"
        download_dir.mkdir(exist_ok=True)
        
        dataset_path = download_dir / "russian-news-paraphrases-2020"
        
        # –°–∫–∞—á–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç, –µ—Å–ª–∏ –µ–≥–æ –µ—â—ë –Ω–µ—Ç
        if not dataset_path.exists():
            print(f"  –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤ {dataset_path}...")
            api.dataset_download_files(
                "viacheslavshalamov/russian-news-paraphrases-2020",
                path=str(dataset_path),
                unzip=True
            )
        
        # –ò—â–µ–º XML —Ñ–∞–π–ª —Å –ø–∞—Ä–∞—Ñ—Ä–∞–∑–∞–º–∏
        xml_file = dataset_path / "Russian-news-paraphrases-2020.xml"
        
        if not xml_file.exists():
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª {xml_file}")
        
        # –ü–∞—Ä—Å–∏–º XML
        tree = ET.parse(xml_file)
        root = tree.getroot()
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_pairs = 0
        label_counts = {0: 0, 1: 0}
        has_title = 0
        has_text = 0
        
        for paraphrase in root.findall(".//paraphrase"):
            entry = {}
            for value in paraphrase.findall("value"):
                name = value.get("name")
                text = value.text or ""
                entry[name] = text
            
            if "class" in entry:
                try:
                    label = int(entry["class"])
                    if label in {0, 1}:
                        total_pairs += 1
                        label_counts[label] += 1
                        
                        if "title_1" in entry and "title_2" in entry:
                            if entry["title_1"] and entry["title_2"]:
                                has_title += 1
                        
                        if "text_1" in entry and "text_2" in entry:
                            if entry["text_1"] and entry["text_2"]:
                                has_text += 1
                except (ValueError, KeyError):
                    continue
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        label_info = {
            "–§–æ—Ä–º–∞—Ç": "XML —Ñ–∞–π–ª —Å –Ω–æ–≤–æ—Å—Ç–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ 2020 –≥–æ–¥–∞",
            "–í—Å–µ–≥–æ —Ä–∞–∑–º–µ—á–µ–Ω–æ": f"{total_pairs:,} (100%)",
            "": "",
            "–î–æ—Å—Ç—É–ø–Ω—ã–µ –ø–æ–ª—è": "",
            "  –ó–∞–≥–æ–ª–æ–≤–∫–∏ (title_1, title_2)": f"{has_title:,} –ø–∞—Ä ({has_title/total_pairs*100:.2f}%)",
            "  –ü–æ–ª–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (text_1, text_2)": f"{has_text:,} –ø–∞—Ä ({has_text/total_pairs*100:.2f}%)",
            "  ": "",
            "–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫": "",
        }
        
        for label, count in sorted(label_counts.items()):
            percentage = (count / total_pairs) * 100
            label_name = "–ø–∞—Ä–∞—Ñ—Ä–∞–∑ (–¥—É–±–ª–∏–∫–∞—Ç)" if label == 1 else "–Ω–µ –ø–∞—Ä–∞—Ñ—Ä–∞–∑"
            label_info[f"  {label} ({label_name})"] = f"{count:,} ({percentage:.2f}%)"
        
        # –°–æ–∑–¥–∞—ë–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–π DataFrame –¥–ª—è –≤—ã–∑–æ–≤–∞ print_dataset_stats
        df = pd.DataFrame({"dummy": range(total_pairs)})
        
        print_dataset_stats("viacheslavshalamov/russian-news-paraphrases-2020 (Kaggle)", df, label_info)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ Kaggle –¥–∞—Ç–∞—Å–µ—Ç–∞: {e}")
        print()


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è."""
    print("\n" + "=" * 80)
    print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ò–°–•–û–î–ù–´–ú –î–ê–¢–ê–°–ï–¢–ê–ú")
    print("=" * 80)
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—é
    setup_auth()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    analyze_ru_paraphraser()
    analyze_opusparcus()
    analyze_leipzig()
    analyze_kaggle_news()
    
    print("=" * 80)
    print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω")
    print("=" * 80)


if __name__ == "__main__":
    main()

